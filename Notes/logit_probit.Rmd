---
title: "Pol Sci 200D Notes: Logistic and Probit Regressions"
author: "Barney Chen"
date: "2025-01-17"
output: html_document
---

\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\logit}{{\rm logit}}
\newcommand{\var}{{\rm Var}}
\newcommand{\cov}{{\rm Cov}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, fig.align = 'center')
```

# Maximum Likelihood Estimation

## Why MLE?
You observe some data ($x_1, \ldots, x_n$, or $\X$), you assume a distribution generates the data but you do not know the parameter $\theta$, so you want to estimate a parameter $\hat \theta$ that makes the observed data most **likely** to have occurred. That is why we call it likelihood ($L(\theta)$). Your job is to estimate the $\hat \theta$ that maximizes the likelihood $\hat \theta = \underset{\theta}{\mathrm{argmax}}\, L(\theta)$. Here are the usual steps to get the maximum likelihood estimator: 

- If we assume the data are i.i.d. (have the same PDF or PMF), the joint likelihood is the product of every $f(x_i | \theta)$: $L(\theta) = \prod_{i=1}^{n} f(x_i | \theta)$. 
- We usually take the log of the likelihood to make the math easier: $\ln L(\theta) = \sum_{i=1}^{n} \ln f(x_i | \theta)$.
- We take the first derivative of the log-likelihood with respect to $\theta$ and set it to zero (first-order condition): solve $\frac{\partial \ln L(\theta)}{\partial \theta} = 0$.

```{r}
set.seed(2025)

# generate some data
dta <- rnorm(1000, mean = 5, sd = 2)

# log-likelihood function
loglik = function(theta, x) {
  mu = theta[1]
  sigma2 = theta[2]
  n = length(x)
  ans = -(n/2)*log(2*pi) - (n/2)*log(sigma2) - 1/(2*sigma2)*sum((x - mu)^2)
  return(-ans) # negative
}

# maximize the log-likelihood function
res <- optim(c(0, 1), loglik, method = "BFGS", hessian = TRUE, x = dta)

# MLE estimates
res$par

# Hessian matrix
res$hessian

# get the SE
sqrt(diag(solve(res$hessian)))
```


## MLE versus OLS
You probably enter this course with previous experience of running `glm()` for your research or using logistic regression in a machine learning prediction/classification setting. OLS is intuitive -- you aim to minimize the sum of squared errors. MLE, on the other hand, seeks to maximize the likelihood (or log-likelihood). Why are they not the same?

Consider a statement: 

> "Least Square Estimation is same as Maximum Likelihood Estimation under a Gaussian model." 

Is this statement true? Yes. From lecture, we know the log-likelihood is $-\frac{n}{2} \log(2\pi \sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \beta_i x_i)^2$. Maximizing it is equivalent to minimizing the sum of squared errors, $\sum_{i=1}^{n} (y_i - \beta_i x_i)^2$.

Let's go back to the real-world situation. If the outcome variable is binary, using OLS violates the homoskedasticity and normality assumptions (of the error terms), and worst of all, the predicted values can be outside the range of $[0,1]$. 

## Properties of Maximum Likelihood Estimators

- Unbiasedness: MLE is only asymptotically unbiased. 
- Efficiency: MLE is asymptotically (uniformly) minimum variance unbiased estimator (MVUE). I will not bother you with the definition of Cramer-Rao Lower Bound (CRLB) for the proof, but if you are interested, check any canonical statistics textbook on GLM. 
- Consistency: MLE of i.i.d. observations is consistent. 
- Asymptotic Normality: $\sqrt{n}(\hat{\theta} - \theta) \xrightarrow{d} N(0, \frac{1}{I(\theta)})$, where $I(\theta)$ is the Fisher Information.

## Logistic Regression 

